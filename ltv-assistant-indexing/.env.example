# Server Configuration
PORT=50052
TCP_PORT=4003
NODE_ENV=development

# Database Configuration (ltv_assistant_indexing_db)
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=root
DB_NAME=ltv_assistant_indexing_db
DB_POOL_MAX=10

# Redis Configuration (for BullMQ)
REDIS_HOST=localhost
REDIS_PORT=6379

# Storage Configuration (S3-Compatible Object Storage)
STORAGE_ENDPOINT=localhost
STORAGE_PORT=9000
STORAGE_ACCESS_KEY=minioadmin
STORAGE_SECRET_KEY=minioadmin
STORAGE_USE_SSL=false
STORAGE_REGION=us-east-1
STORAGE_BUCKET=documents
STORAGE_FORCE_PATH_STYLE=true

# Load Stage Configuration
# Streaming threshold: Files larger than this will be streamed (in MB, default: 50)
LOAD_STREAMING_THRESHOLD=50

# Chunk size for streaming: Size of each chunk when streaming (in bytes, default: 64KB = 65536)
LOAD_CHUNK_SIZE=65536

# Temporary directory for large file streaming
LOAD_TEMP_DIR=/tmp/indexing

# Datasource Service (TCP) - for file metadata retrieval
DATASOURCE_SERVICE_HOST=localhost
DATASOURCE_SERVICE_PORT=4001

# ============================================
# LLM Provider Configuration
# ============================================

# Provider Selection
# Options: openai | google | anthropic | ollama
LLM_PROVIDER=ollama
EMBEDDING_PROVIDER=ollama

# --------------------------------------------
# OpenAI Configuration
# --------------------------------------------
# Required if using OpenAI
OPENAI_API_KEY=
# Custom base URL (optional)
OPENAI_BASE_URL=https://api.openai.com/v1
# Chat model (default: gpt-4o)
OPENAI_CHAT_MODEL=gpt-4o
# Embedding model
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# --------------------------------------------
# Google Gemini Configuration
# --------------------------------------------
# Required if using Google
GOOGLE_API_KEY=
# Chat model (default: gemini-2.5-flash-lite)
GOOGLE_CHAT_MODEL=gemini-2.5-flash-lite
# Embedding model
GOOGLE_EMBEDDING_MODEL=text-embedding-004

# --------------------------------------------
# Anthropic Claude Configuration
# --------------------------------------------
# Required if using Anthropic
ANTHROPIC_API_KEY=
# Custom base URL (optional)
ANTHROPIC_BASE_URL=https://api.anthropic.com
# Chat model
ANTHROPIC_CHAT_MODEL=claude-sonnet-4-5-20250929

# --------------------------------------------
# Ollama Configuration (DEFAULT for embeddings)
# --------------------------------------------
# Ollama server URL
OLLAMA_BASE_URL=http://localhost:11434
# Chat model for summaries/questions
OLLAMA_CHAT_MODEL=gemma3:1b
# DEFAULT embedding model (as specified)
OLLAMA_EMBEDDING_MODEL=bge-m3:567m

# ============================================
# Enrichment Feature Toggles
# ============================================

# Entity Extraction (Algorithmic)
# Default: true
ENTITY_EXTRACTION_ENABLED=true
# Methods: regex, nlp
ENTITY_EXTRACTION_METHODS=regex,nlp
# Minimum confidence threshold
ENTITY_MIN_CONFIDENCE=0.5

# Keyword Extraction
# Options: tfidf | llm | none (Default: tfidf)
KEYWORD_EXTRACTION_METHOD=tfidf
# Top K keywords to extract
KEYWORD_TOP_K=10

# LLM-based Summary Generation (Optional)
# Default: false (có chi phí)
SUMMARY_GENERATION_ENABLED=false
# Provider for summaries (optional)
# Options: openai | google | anthropic | ollama
# If not set, uses LLM_PROVIDER (global default)
SUMMARY_GENERATION_PROVIDER=
# Model for summaries (optional)
# If not set, uses provider's default model
SUMMARY_GENERATION_MODEL=
# Max tokens for summary
SUMMARY_MAX_TOKENS=100
# Temperature (0.0-1.0)
SUMMARY_TEMPERATURE=0.3
# Chunks per batch (reduced from 10 to 5 for better rate limiting)
SUMMARY_BATCH_SIZE=5
# Timeout in seconds (LangChain built-in timeout)
SUMMARY_TIMEOUT=20

# Summary Fallback Configuration
# If primary provider fails (rate limit, timeout, error), try fallback
# Recommended: Use local Ollama as fallback (always available, no rate limits)
SUMMARY_FALLBACK_ENABLED=true
SUMMARY_FALLBACK_PROVIDER=ollama
SUMMARY_FALLBACK_MODEL=gemma3:1b

# Hypothetical Questions Generation (Optional, Multi-Vector)
# Default: false (có chi phí)
HYPOTHETICAL_QUESTIONS_ENABLED=false
# Provider for questions (optional)
# Options: openai | google | anthropic | ollama
# If not set, uses LLM_PROVIDER (global default)
HYPOTHETICAL_QUESTIONS_PROVIDER=
# Model for questions (optional)
# If not set, uses provider's default model
HYPOTHETICAL_QUESTIONS_MODEL=
# Questions per chunk (3-5)
HYPOTHETICAL_QUESTIONS_COUNT=5
# Max tokens for questions
HYPOTHETICAL_QUESTIONS_MAX_TOKENS=150
# Higher for diversity
HYPOTHETICAL_QUESTIONS_TEMPERATURE=0.7
# Chunks per batch (reduced from 10 to 5 for better rate limiting)
HYPOTHETICAL_QUESTIONS_BATCH_SIZE=5
# Timeout in seconds (LangChain built-in timeout)
HYPOTHETICAL_QUESTIONS_TIMEOUT=20

# Hypothetical Questions Fallback Configuration
# If primary provider fails (rate limit, timeout, error), try fallback
# Recommended: Use local Ollama as fallback (always available, no rate limits)
HYPOTHETICAL_QUESTIONS_FALLBACK_ENABLED=true
HYPOTHETICAL_QUESTIONS_FALLBACK_PROVIDER=ollama
HYPOTHETICAL_QUESTIONS_FALLBACK_MODEL=gemma3:1b

# ============================================
# Performance Configuration
# ============================================
# Parallel chunk processing
ENRICH_PARALLEL_CHUNKS=10
# Overall stage timeout
ENRICH_TIMEOUT_MS=60000

# ============================================
# LLM Retry Configuration
# ============================================
# Max retry attempts for failed LLM requests
LLM_MAX_RETRIES=3
# Retry delay in milliseconds (exponential backoff: 2s, 4s, 8s)
LLM_RETRY_DELAY_MS=2000

# ============================================
# Embedding Stage Configuration
# ============================================
# Batch size: Number of chunks to embed per batch (default: 24)
# Lower = More stable, Higher = Faster but may cause timeouts
EMBEDDING_BATCH_SIZE=12

# Max concurrent batches: How many batches to process simultaneously (default: 2)
# Lower = Less load on Ollama, Higher = Faster but may cause failures
EMBEDDING_MAX_CONCURRENT_BATCHES=1

# Max retry attempts for failed embeddings (default: 3)
EMBEDDING_MAX_RETRIES=3

# Retry delay in milliseconds (default: 1500)
# Exponential backoff: 1500ms, 3000ms, 6000ms
EMBEDDING_RETRY_DELAY_MS=3000

# Timeout per embedding request in milliseconds (default: 60000)
# Increase if using heavy models or slow hardware
EMBEDDING_TIMEOUT_MS=90000

# ============================================
# Vector Database Configuration (Qdrant - for Persist stage)
# ============================================
# Qdrant server URL (HTTP API)
# Default: http://localhost:6333
QDRANT_URL=http://localhost:6333
# Note: Collection names are hardcoded in QdrantInitService:
#   - documents_children (child chunks - primary search)
#   - documents_summaries (summaries - multi-vector retrieval)
#   - documents_questions (hypothetical questions - multi-vector retrieval)

# Optional: Qdrant API key (for production with authentication)
# QDRANT_API_KEY=your-api-key-here

# ============================================
# Knowledge Graph Configuration (Neo4j - for Persist stage)
# ============================================
# Neo4j Bolt protocol URI
# Default: bolt://localhost:7687
NEO4J_URI=bolt://localhost:7687

# Neo4j username
# Default: neo4j
NEO4J_USER=neo4j

# Neo4j password
# Default: neo4j123 (IMPORTANT: Change this in production!)
NEO4J_PASSWORD=neo4j123

# Note: Collections and constraints are auto-created on server startup by:
# - QdrantInitService: Creates 3 collections with named vectors (dense + sparse)
# - Neo4jInitService: Creates constraints and indexes for performance

# Logging Configuration
SERVICE_NAME=ltv-assistant-indexing
LOG_LEVEL=info
LOG_DIR=/Users/tohainam/Desktop/work/ltv-assistant/logs
APP_VERSION=1.0.0

# OpenTelemetry Tracing Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces

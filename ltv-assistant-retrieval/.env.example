# Server Configuration
PORT=50053
TCP_PORT=4005
NODE_ENV=development

# Database Configuration (ltv_assistant_indexing_db)
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=root
DB_NAME=ltv_assistant_indexing_db
DB_POOL_MAX=10

# Redis Configuration (for Cache & Checkpointer)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_CACHE_TTL=3600
REDIS_URL=redis://localhost:6379

# Datasource Service Configuration (TCP)
DATASOURCE_SERVICE_HOST=localhost
DATASOURCE_SERVICE_TCP_PORT=4004

# Auth Service Configuration (TCP)
AUTH_SERVICE_HOST=localhost
AUTH_SERVICE_TCP_PORT=4001

# ============================================
# Vector & Graph Database Configuration
# ============================================

# Qdrant Vector Database
QDRANT_URL=http://localhost:6333
# Optional: Qdrant API key (for production)
# QDRANT_API_KEY=

# Neo4j Knowledge Graph
NEO4J_ENABLED=true
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=neo4j123

# ============================================
# Reranking Service Configuration
# ============================================

# BGE-Reranker Service (TEI)
TEI_RERANKER_URL=http://localhost:6201
TEI_RERANKER_TIMEOUT=30000

# Reranking Score Threshold
# Only accept results with rerank score > threshold
# BGE-Reranker-v2-m3 produces raw scores (can be negative for irrelevant results)
# Default: 0.0 (filter out negative/irrelevant scores)
# Higher values (e.g., 0.3) = more aggressive filtering
RERANK_SCORE_THRESHOLD=0.3

# Reranking Fallback Configuration
# Number of top results to return when all results are filtered by threshold
# If threshold filters out all results, return top N from reranked results anyway
# Default: 3
RERANK_FALLBACK_COUNT=3

# ============================================
# LLM Provider Configuration
# ============================================

# Global Provider Selection
# Options: openai | google | anthropic | ollama
LLM_PROVIDER=ollama
EMBEDDING_PROVIDER=ollama

# --------------------------------------------
# OpenAI Configuration
# --------------------------------------------
# Required if using OpenAI
OPENAI_API_KEY=
# Custom base URL (optional)
OPENAI_BASE_URL=https://api.openai.com/v1
# Chat model (default: gpt-4o)
OPENAI_CHAT_MODEL=gpt-4o
# Embedding model (default: text-embedding-3-small)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# --------------------------------------------
# Google Gemini Configuration
# --------------------------------------------
# Required if using Google
GOOGLE_API_KEY=
# Chat model (default: gemini-2.5-flash-lite)
GOOGLE_CHAT_MODEL=gemini-2.5-flash-lite
# Embedding model (default: text-embedding-004)
GOOGLE_EMBEDDING_MODEL=text-embedding-004

# --------------------------------------------
# Anthropic Claude Configuration
# --------------------------------------------
# Required if using Anthropic
ANTHROPIC_API_KEY=
# Custom base URL (optional)
ANTHROPIC_BASE_URL=https://api.anthropic.com
# Chat model (default: claude-sonnet-4-5-20250929)
ANTHROPIC_CHAT_MODEL=claude-sonnet-4-5-20250929

# --------------------------------------------
# Ollama Configuration (DEFAULT - Local & Free)
# --------------------------------------------
# Ollama server URL
OLLAMA_BASE_URL=http://localhost:11434
# Chat model for query transformations
OLLAMA_CHAT_MODEL=gemma3:1b
# Embedding model (MUST match indexing service)
OLLAMA_EMBEDDING_MODEL=bge-m3:567m

# ============================================
# Query Transformation Configuration
# ============================================

# --------------------------------------------
# Query Reformulation (3-5 variations)
# --------------------------------------------
# Provider override (optional)
# If not set, uses LLM_PROVIDER
QUERY_REFORMULATION_PROVIDER=
# Model override (optional)
# If not set, uses provider's default chat model
QUERY_REFORMULATION_MODEL=
# Temperature for diversity (0.7 recommended)
QUERY_REFORMULATION_TEMPERATURE=0.7
# Max tokens per reformulation
QUERY_REFORMULATION_MAX_TOKENS=200
# Max retry attempts
QUERY_REFORMULATION_MAX_RETRIES=2
# Timeout in milliseconds
QUERY_REFORMULATION_TIMEOUT=10000

# Fallback Configuration
QUERY_REFORMULATION_FALLBACK_ENABLED=true
QUERY_REFORMULATION_FALLBACK_PROVIDER=ollama
QUERY_REFORMULATION_FALLBACK_MODEL=gemma3:1b

# --------------------------------------------
# Query Rewrite (Clarify intent)
# --------------------------------------------
# Provider override (optional)
QUERY_REWRITE_PROVIDER=
# Model override (optional)
QUERY_REWRITE_MODEL=
# Temperature for precision (0.3 recommended)
QUERY_REWRITE_TEMPERATURE=0.3
# Max tokens
QUERY_REWRITE_MAX_TOKENS=150
# Max retry attempts
QUERY_REWRITE_MAX_RETRIES=2
# Timeout in milliseconds
QUERY_REWRITE_TIMEOUT=10000

# Fallback Configuration
QUERY_REWRITE_FALLBACK_ENABLED=true
QUERY_REWRITE_FALLBACK_PROVIDER=ollama
QUERY_REWRITE_FALLBACK_MODEL=gemma3:1b

# --------------------------------------------
# HyDE (Hypothetical Document Embeddings)
# --------------------------------------------
# Provider override (optional)
HYDE_PROVIDER=
# Model override (optional)
HYDE_MODEL=
# Temperature for realistic generation (0.5 recommended)
HYDE_TEMPERATURE=0.5
# Max tokens for hypothetical document
HYDE_MAX_TOKENS=250
# Max retry attempts
HYDE_MAX_RETRIES=2
# Timeout in milliseconds
HYDE_TIMEOUT=10000

# Fallback Configuration
HYDE_FALLBACK_ENABLED=true
HYDE_FALLBACK_PROVIDER=ollama
HYDE_FALLBACK_MODEL=gemma3:1b

# --------------------------------------------
# Query Decomposition (Break into sub-queries)
# --------------------------------------------
# Provider override (optional)
QUERY_DECOMPOSITION_PROVIDER=
# Model override (optional)
QUERY_DECOMPOSITION_MODEL=
# Temperature (0.4 recommended)
QUERY_DECOMPOSITION_TEMPERATURE=0.4
# Max tokens
QUERY_DECOMPOSITION_MAX_TOKENS=200
# Max retry attempts
QUERY_DECOMPOSITION_MAX_RETRIES=2
# Timeout in milliseconds
QUERY_DECOMPOSITION_TIMEOUT=10000

# Fallback Configuration
QUERY_DECOMPOSITION_FALLBACK_ENABLED=true
QUERY_DECOMPOSITION_FALLBACK_PROVIDER=ollama
QUERY_DECOMPOSITION_FALLBACK_MODEL=gemma3:1b

# ============================================
# Retrieval Configuration
# ============================================

# Default top-K results to retrieve
TOP_K_DEFAULT=10

# ============================================
# Phase 1.5: Semantic Cache Configuration
# ============================================

# Cache TTL in seconds (default: 3600 = 1 hour)
CACHE_TTL=3600

# Semantic similarity threshold for cache hits (0.95 = very strict)
# Higher = more conservative (less cache hits, more accuracy)
# Lower = more lenient (more cache hits, less accuracy)
CACHE_SIMILARITY_THRESHOLD=0.95

# Cache collection name in Qdrant
CACHE_COLLECTION_NAME=query_cache_public

# Enable/disable semantic cache globally
# Individual requests can override with useCache parameter
CACHE_ENABLED=true

# Cache cleanup cron schedule (default: every hour)
# Cron format: '0 * * * *' = every hour at minute 0
CACHE_CLEANUP_CRON=0 * * * *

# ============================================
# Adaptive Loop Configuration
# Max retry iterations if context quality is insufficient
MAX_RETRY_ITERATIONS=3
# Sufficiency threshold (0-1)
SUFFICIENCY_THRESHOLD=0.6
# High quality score threshold
HIGH_QUALITY_THRESHOLD=0.7
# Minimum contexts required
MIN_CONTEXTS=3

# ============================================
# Phase 2: Answer Generation Configuration
# (FUTURE - Not implemented in Phase 1)
# ============================================

# Answer Generation Provider
# Options: openai | google | anthropic | ollama
ANSWER_GENERATION_PROVIDER=
# Model for answer generation
ANSWER_GENERATION_MODEL=
# Temperature (0.7 recommended for creative answers)
ANSWER_GENERATION_TEMPERATURE=0.7
# Max tokens for answer
ANSWER_GENERATION_MAX_TOKENS=500

# Streaming Configuration
ANSWER_STREAMING_ENABLED=false

# Hallucination Detection
HALLUCINATION_DETECTION_ENABLED=false

# ============================================
# Performance Configuration
# ============================================

# Query transformation parallel execution timeout
QUERY_TRANSFORMATION_TOTAL_TIMEOUT=30000

# Embedding batch configuration
EMBEDDING_BATCH_SIZE=24
EMBEDDING_MAX_CONCURRENT_BATCHES=2
EMBEDDING_MAX_RETRIES=3
EMBEDDING_RETRY_DELAY_MS=1500

# ============================================
# Logging Configuration
# ============================================

SERVICE_NAME=ltv-assistant-retrieval
LOG_LEVEL=info
LOG_DIR=/Users/tohainam/Desktop/work/ltv-assistant/logs
APP_VERSION=1.0.0

# OpenTelemetry Tracing Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces

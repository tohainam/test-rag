# Káº¿ Hoáº¡ch Triá»ƒn Khai Chunk Stage

**PhiÃªn báº£n:** 2.0
**Cáº­p nháº­t láº§n cuá»‘i:** 2025-11-03
**Tráº¡ng thÃ¡i:** Production Ready
**Giai Ä‘oáº¡n:** 4/7 (Chunk)

## ğŸ¯ NGUYÃŠN Táº®C Cá»T LÃ•I: 100% Báº¢O TOÃ€N Dá»® LIá»†U

**Cam káº¿t:** KhÃ´ng bá» qua báº¥t ká»³ chunk nÃ o, khÃ´ng cáº¯t ngáº¯n ná»™i dung, khÃ´ng máº¥t dá»¯ liá»‡u.

---

## Má»¥c lá»¥c

1. [Tá»•ng quan](#tá»•ng-quan)
2. [Chiáº¿n lÆ°á»£c 100% Data Retention](#chiáº¿n-lÆ°á»£c-100-data-retention)
3. [Tá»‘i Æ°u hÃ³a Token Size](#tá»‘i-Æ°u-hÃ³a-token-size)
4. [YÃªu cáº§u nghiá»‡p vá»¥](#yÃªu-cáº§u-nghiá»‡p-vá»¥)
5. [Äáº·c táº£ ká»¹ thuáº­t](#Ä‘áº·c-táº£-ká»¹-thuáº­t)
6. [Chiáº¿n lÆ°á»£c xá»­ lÃ½ lá»—i](#chiáº¿n-lÆ°á»£c-xá»­-lÃ½-lá»—i)
7. [YÃªu cáº§u hiá»‡u nÄƒng](#yÃªu-cáº§u-hiá»‡u-nÄƒng)
8. [TiÃªu chÃ­ thÃ nh cÃ´ng](#tiÃªu-chÃ­-thÃ nh-cÃ´ng)
9. [Database Schema](#database-schema)
10. [TÃ­ch há»£p LangGraph](#tÃ­ch-há»£p-langgraph)

---

## Tá»•ng quan

### Má»¥c Ä‘Ã­ch

Chunk Stage lÃ  **giai Ä‘oáº¡n thá»© 4 trong pipeline indexing** (7 giai Ä‘oáº¡n), chá»‹u trÃ¡ch nhiá»‡m chia documents thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ cÃ³ thá»ƒ tÃ¬m kiáº¿m Ä‘Æ°á»£c báº±ng chiáº¿n lÆ°á»£c **Small-to-Big Chunking**.

**Vá»‹ trÃ­ trong pipeline:**
```
Load â†’ Parse â†’ Structure â†’ [CHUNK] â†’ Enrich â†’ Embed â†’ Persist
                              ^^^^
                          Giai Ä‘oáº¡n nÃ y
```

**Chá»©c nÄƒng chÃ­nh:**
- Chuyá»ƒn Ä‘á»•i tá»« "structured sections" â†’ "hierarchical chunks with lineage"
- Sá»­ dá»¥ng LangChain.js `RecursiveCharacterTextSplitter` vá»›i **token-based length function**
- Äáº£m báº£o **100% Ä‘á»™ chÃ­nh xÃ¡c** vá» token count
- **Báº£o toÃ n 100%** dá»¯ liá»‡u Ä‘áº§u vÃ o

### Insight quan trá»ng

**Sá»± khÃ¡c biá»‡t giá»¯a Parent vÃ  Child Chunks:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PARENT CHUNKS (LÆ°u trá»¯ ngá»¯ cáº£nh)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ KHÃ”NG cáº§n embedding                       â”‚
â”‚ â€¢ KHÃ”NG bá»‹ giá»›i háº¡n embedding model        â”‚
â”‚ â€¢ Chá»‰ cáº§n vá»«a LLM context window           â”‚
â”‚ â€¢ Má»¥c tiÃªu: 1,800 tokens                   â”‚
â”‚ â€¢ Cháº¥p nháº­n: lÃªn Ä‘áº¿n 10,000 tokens         â”‚
â”‚ â€¢ Vai trÃ²: Cung cáº¥p context Ä‘áº§y Ä‘á»§ cho LLM â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHILD CHUNKS (ÄÆ¡n vá»‹ tÃ¬m kiáº¿m)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Báº®T BUá»˜C pháº£i embedding                   â”‚
â”‚ â€¢ Báº®T BUá»˜C <= 8,191 tokens (giá»›i háº¡n model)â”‚
â”‚ â€¢ Má»¥c tiÃªu: 512 tokens (tá»‘i Æ°u)            â”‚
â”‚ â€¢ Vai trÃ²: Vector search, tÃ¬m kiáº¿m chÃ­nh xÃ¡câ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Luá»“ng hoáº¡t Ä‘á»™ng:**
1. User query â†’ Embedding query
2. Vector search trong **child chunks** (tÃ¬m chÃ­nh xÃ¡c)
3. Láº¥y child chunk match â†’ Tra ngÆ°á»£c lÃªn **parent chunk**
4. Feed parent chunk vÃ o LLM (context Ä‘áº§y Ä‘á»§)

### Pháº¡m vi

**Trong pháº¡m vi:**
- âœ… Chia sections â†’ parent chunks (1,800 tokens)
- âœ… Chia parent chunks â†’ child chunks (512 tokens)
- âœ… Token-based lengthFunction vá»›i tiktoken (chÃ­nh xÃ¡c 100%)
- âœ… Theo dÃµi lineage parent-child (má»—i child â†’ 1 parent)
- âœ… **100% data retention** (khÃ´ng bá», khÃ´ng cáº¯t)
- âœ… TÃ­nh token counts chÃ­nh xÃ¡c (cl100k_base encoding)
- âœ… Táº¡o chunk IDs á»•n Ä‘á»‹nh (MD5 hash content)
- âœ… Báº£o toÃ n metadata hierarchy
- âœ… Validation toÃ n diá»‡n

**NgoÃ i pháº¡m vi:**
- âŒ TrÃ­ch xuáº¥t entities â†’ Enrich Stage
- âŒ Táº¡o embeddings â†’ Embed Stage
- âŒ LÆ°u trá»¯ chunks â†’ Persist Stage
- âŒ TÃ³m táº¯t content â†’ Enrich Stage

---

## Chiáº¿n lÆ°á»£c 100% Data Retention

### 1. PhÃ¢n biá»‡t rÃµ rÃ ng Parent vs Child

**Váº¥n Ä‘á» cá»§a thiáº¿t káº¿ cÅ©:**
- Ãp dá»¥ng cÃ¹ng constraints cho cáº£ parent vÃ  child
- Parent bá»‹ giá»›i háº¡n 1,400 tokens â†’ Máº¥t ngá»¯ cáº£nh
- Lo ngáº¡i embedding limit cho parent â†’ KhÃ´ng cáº§n thiáº¿t!

**Giáº£i phÃ¡p má»›i:**
- **Parent chunks:** KhÃ´ng embed â†’ KhÃ´ng cáº§n lo embedding limit
- **Child chunks:** Pháº£i embed â†’ Báº¯t buá»™c tuÃ¢n thá»§ limit 8,191 tokens

### 2. Token-based Length Function

**CÃ¡ch cÅ© (khÃ´ng chÃ­nh xÃ¡c):**
```typescript
// âŒ Sá»¬ Dá»¤NG CHARACTER-BASED (Æ¯á»šC LÆ¯á»¢NG)
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1600,  // characters
  lengthFunction: (text) => text.length,  // Äáº¿m kÃ½ tá»±
});

// Váº¥n Ä‘á»:
// - 1600 chars cÃ³ thá»ƒ = 300-600 tokens (tÃ¹y loáº¡i text)
// - Tiáº¿ng Anh: ~4 chars/token
// - Tiáº¿ng Viá»‡t cÃ³ dáº¥u: ~5-6 chars/token
// - Code: ~3 chars/token
// - KhÃ´ng á»•n Ä‘á»‹nh!
```

**CÃ¡ch má»›i (100% chÃ­nh xÃ¡c):**
```typescript
// âœ… Sá»¬ Dá»¤NG TOKEN-BASED (CHÃNH XÃC TUYá»†T Äá»I)
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 512,  // TOKENS trá»±c tiáº¿p
  lengthFunction: (text) => tokenCounter.countTokens(text),  // Äáº¿m tokens tháº­t
});

// Káº¿t quáº£:
// - Má»—i chunk CHÃNH XÃC ~512 tokens
// - KhÃ´ng phá»¥ thuá»™c loáº¡i text
// - á»”n Ä‘á»‹nh 100%
```

### 3. Chiáº¿n lÆ°á»£c Zero Data Loss

| TÃ¬nh huá»‘ng | CÃ¡ch xá»­ lÃ½ | Máº¥t dá»¯ liá»‡u? |
|-----------|-----------|--------------|
| Section nhá» (<100 tokens) | Giá»¯ nguyÃªn lÃ m 1 parent chunk | âŒ KhÃ´ng |
| Section lá»›n (>10,000 tokens) | Split thÃ nh nhiá»u parents | âŒ KhÃ´ng |
| Parent chunk nhá» (<100 tokens) | Giá»¯ nguyÃªn lÃ m 1 child chunk | âŒ KhÃ´ng |
| Parent chunk lá»›n | Split thÃ nh nhiá»u children (512 tokens/chunk) | âŒ KhÃ´ng |
| Child > 8,191 tokens (hiáº¿m) | Force split theo tá»« â†’ Validate láº¡i | âŒ KhÃ´ng |
| 1 section chunking lá»—i | Log lá»—i, tiáº¿p tá»¥c sections khÃ¡c | âš ï¸ Má»™t pháº§n (cÃ³ log) |
| Táº¤T Cáº¢ sections lá»—i | Throw error, reject document | âœ… HÃ nh vi mong Ä‘á»£i |

### 4. Bá»‘n lá»›p Validation

```
Lá»›p 1: Token Count Validation
â”œâ”€ Parent: Cáº£nh bÃ¡o náº¿u > 10,000 tokens (váº«n cháº¥p nháº­n)
â””â”€ Child: Lá»—i náº¿u > 8,191 tokens (buá»™c re-split)

Lá»›p 2: Content Preservation Validation
â”œâ”€ So sÃ¡nh tá»•ng kÃ½ tá»± input vs output
â””â”€ Cáº£nh bÃ¡o náº¿u output < input Ã— 0.95

Lá»›p 3: Lineage Validation
â”œâ”€ Má»—i child PHáº¢I cÃ³ Ä‘Ãºng 1 parent
â””â”€ KhÃ´ng cÃ³ child "má»“ cÃ´i"

Lá»›p 4: Coverage Validation
â”œâ”€ ToÃ n bá»™ content parent xuáº¥t hiá»‡n trong children
â””â”€ Validate tá»•ng tokens (cÃ³ tÃ­nh overlap)
```

---

## Tá»‘i Æ°u hÃ³a Token Size

### Parent Chunks: 1,800 tokens

**LÃ½ do chá»n 1,800:**

1. **NghiÃªn cá»©u vá» LLM comprehension:**
   - CÃ¡c nghiÃªn cá»©u cho tháº¥y LLM hiá»ƒu tá»‘t nháº¥t á»Ÿ 1,500-2,500 tokens
   - 1,800 tokens â‰ˆ 1-2 trang text (Ä‘á»™ dÃ i lÃ½ tÆ°á»Ÿng Ä‘á»ƒ Ä‘á»c hiá»ƒu)

2. **Context window cá»§a LLMs:**
   - GPT-4 Turbo: 128K tokens
   - GPT-4: 8K tokens
   - 1,800 tokens chá»‰ chiáº¿m ~1.4% cá»§a GPT-4 Turbo â†’ Ráº¥t nhá»

3. **KhÃ´ng bá»‹ giá»›i háº¡n embedding:**
   - Parent KHÃ”NG embed â†’ KhÃ´ng quan tÃ¢m giá»›i háº¡n 8,191 tokens
   - CÃ³ thá»ƒ lá»›n hÆ¡n náº¿u cáº§n (accept lÃªn Ä‘áº¿n 10,000 tokens)

4. **Practical limit:**
   - User hiáº¿m khi cáº§n >2,000 tokens context
   - Database performance (text dÃ i = query cháº­m)

**Táº¡i sao KHÃ”NG lá»›n hÆ¡n? (vÃ­ dá»¥ 5,000 tokens)**
- âŒ LLM attention quality giáº£m vá»›i context ráº¥t dÃ i
- âŒ Database query cháº­m
- âŒ Memory usage cao
- âŒ Thá»±c táº¿ khÃ´ng cáº§n thiáº¿t

**Táº¡i sao KHÃ”NG nhá» hÆ¡n? (vÃ­ dá»¥ 800 tokens)**
- âŒ Máº¥t context quan trá»ng
- âŒ Nhiá»u chunks hÆ¡n = overhead DB cao
- âŒ ThÃ´ng tin bá»‹ phÃ¢n máº£nh

---

### Child Chunks: 512 tokens

**LÃ½ do chá»n 512:**

1. **Giá»›i háº¡n cá»§a Embedding Models:**
   - text-embedding-ada-002: 8,191 tokens
   - text-embedding-3-small: 8,191 tokens
   - text-embedding-3-large: 8,191 tokens
   - 512 << 8,191 â†’ An toÃ n tuyá»‡t Ä‘á»‘i

2. **NghiÃªn cá»©u vá» Retrieval Quality:**
   - CÃ¡c nghiÃªn cá»©u RAG cho tháº¥y 400-600 tokens tá»‘i Æ°u cho semantic search
   - QuÃ¡ nhá» (<256): Máº¥t context
   - QuÃ¡ lá»›n (>1024): Semantic signal bá»‹ pha loÃ£ng

3. **Power of 2:**
   - 512 = 2^9 â†’ Hiá»‡u quáº£ cho nhiá»u ML frameworks
   - Benchmark standard trong nhiá»u RAG papers

4. **Semantic Coherence:**
   - 512 tokens â‰ˆ 1-2 Ä‘oáº¡n vÄƒn
   - Äá»§ lá»›n Ä‘á»ƒ chá»©a má»™t concept hoÃ n chá»‰nh
   - KhÃ´ng quÃ¡ lá»›n Ä‘á»ƒ máº¥t focus

5. **Chi phÃ­:**
   - Embedding cost tá»· lá»‡ vá»›i sá»‘ tokens
   - 512 tokens â†’ Chi phÃ­ há»£p lÃ½
   - Nhá» hÆ¡n 400 â†’ Nhiá»u chunks hÆ¡n â†’ Search cháº­m

**Táº¡i sao KHÃ”NG 256 tokens?**
- âŒ QuÃ¡ nhá» â†’ Máº¥t context
- âŒ Nhiá»u chunks gáº¥p Ä‘Ã´i â†’ Search cháº­m
- âŒ ThÃ´ng tin quÃ¡ phÃ¢n máº£nh

**Táº¡i sao KHÃ”NG 1,024 tokens?**
- âŒ Semantic signal pha loÃ£ng
- âŒ Retrieval kÃ©m chÃ­nh xÃ¡c hÆ¡n
- âŒ Chi phÃ­ embedding cao hÆ¡n

---

### Chiáº¿n lÆ°á»£c Overlap

**Parent Overlap: 180 tokens (10%)**

**LÃ½ do:**
- 10% cá»§a 1,800 = 180 tokens
- â‰ˆ 1-2 cÃ¢u overlap
- NgÄƒn cháº·n viá»‡c cáº¯t Ä‘á»©t concepts á»Ÿ ranh giá»›i
- KhÃ´ng quÃ¡ lá»›n (lÃ£ng phÃ­ storage)

**VÃ­ dá»¥:**
```
Chunk 1: "... há»‡ thá»‘ng authentication sá»­ dá»¥ng JWT tokens ..."
                                                        ^^^^^^
                                                        Overlap
Chunk 2: "... JWT tokens. NÃ³ há»— trá»£ thuáº­t toÃ¡n RS256 ..."
         ^^^^^^^^^^^
         Overlap
```
â†’ Chunk 2 váº«n cÃ³ context vá» JWT tokens

**Child Overlap: 50 tokens (~10%)**

**LÃ½ do:**
- 10% cá»§a 512 â‰ˆ 50 tokens
- â‰ˆ 1 cÃ¢u overlap
- Nhá» hÆ¡n parent overlap vÃ¬ children Ä‘Ã£ náº±m trong parent context

---

### Báº£ng so sÃ¡nh V1.0 vs V2.0

| Chá»‰ sá»‘ | V1.0 (CÅ©) | V2.0 (Má»›i) | Cáº£i thiá»‡n |
|--------|-----------|------------|-----------|
| **Parent Size** | 1,200 tokens (Æ°á»›c lÆ°á»£ng) | 1,800 tokens (chÃ­nh xÃ¡c) | +50% context |
| **Parent Max** | 1,680 tokens (hard limit) | 10,000 tokens (cáº£nh bÃ¡o) | Linh hoáº¡t hÆ¡n |
| **Parent Precision** | Â±30% (char-based) | 100% (token-based) | ChÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i |
| **Child Size** | 400 tokens (Æ°á»›c lÆ°á»£ng) | 512 tokens (chÃ­nh xÃ¡c) | +28% má»—i chunk |
| **Child Max** | 600 tokens â†’ accept | 8,191 tokens â†’ force split | KhÃ´ng fail embedding |
| **Data Loss** | 0.01% (skip chunks nhá») | **0%** (khÃ´ng skip) | **100% retention** |
| **Äá»™ chÃ­nh xÃ¡c** | ~70-80% | ~99.9% | Cao nháº¥t |
| **Xá»­ lÃ½ edge cases** | Throw errors | Graceful degradation | Production ready |

---

## YÃªu cáº§u nghiá»‡p vá»¥

### YN-1: Táº¡o Parent Chunks vá»›i 100% Data Retention

**Äá»™ Æ°u tiÃªn:** P0 (Cá»±c ká»³ quan trá»ng)

**MÃ´ táº£:**
Há»‡ thá»‘ng pháº£i táº¡o parent chunks vá»›i Ä‘á»™ chÃ­nh xÃ¡c token-based vÃ  KHÃ”NG Máº¤T báº¥t ká»³ dá»¯ liá»‡u nÃ o.

**TiÃªu chÃ­ cháº¥p nháº­n:**
- âœ… Sá»­ dá»¥ng tiktoken lÃ m lengthFunction (token-based)
- âœ… Má»¥c tiÃªu: 1,800 tokens/chunk
- âœ… Overlap: 180 tokens (10%)
- âœ… Giá»›i háº¡n má»m: Cáº£nh bÃ¡o náº¿u > 10,000 tokens nhÆ°ng VáºªN CHáº¤P NHáº¬N
- âœ… KhÃ´ng cÃ³ hard limits â†’ KhÃ´ng reject chunks
- âœ… Báº£o toÃ n metadata: sectionPath, pageNumber, hierarchy
- âœ… Encoding: cl100k_base (GPT-3.5/4 compatible)
- âœ… Xá»­ lÃ½ sections nhá»: <100 tokens â†’ giá»¯ nguyÃªn lÃ m 1 parent chunk

**CÃ¡c test case:**

```typescript
// Test 1: Section bÃ¬nh thÆ°á»ng
section = { content: "...", tokens: 5000 }
result = splitSectionIntoParents(section)

expect(result).toHaveLength(3) // ~1800 Ã— 3 = 5400 (cÃ³ overlap)
expect(result.every(c => c.tokens >= 1500 && c.tokens <= 2200)).toBe(true)

// Test 2: Section ráº¥t nhá»
section = { content: "Short text", tokens: 50 }
result = splitSectionIntoParents(section)

expect(result).toHaveLength(1)
expect(result[0].tokens).toBe(50) // Giá»¯ nguyÃªn

// Test 3: Section ráº¥t lá»›n
section = { content: "...", tokens: 100000 }
result = splitSectionIntoParents(section)

expect(result).toHaveLength(>= 50)
expect(result.every(c => c.tokens <= 10000)).toBe(true) // Táº¥t cáº£ Ä‘á»u cháº¥p nháº­n

// Test 4: Báº£o toÃ n dá»¯ liá»‡u
inputTokens = countTokens(section.content)
outputTokens = sum(result.map(c => c.tokens))

expect(outputTokens).toBeGreaterThanOrEqual(inputTokens) // Do overlap
```

---

### YN-2: Táº¡o Child Chunks vá»›i RÃ ng buá»™c Embedding

**Äá»™ Æ°u tiÃªn:** P0 (Cá»±c ká»³ quan trá»ng)

**MÃ´ táº£:**
Há»‡ thá»‘ng pháº£i chia má»—i parent chunk thÃ nh child chunks, Ä‘áº£m báº£o KHÃ”NG VÆ¯á»¢T QUÃ giá»›i háº¡n embedding model.

**TiÃªu chÃ­ cháº¥p nháº­n:**
- âœ… Sá»­ dá»¥ng tiktoken lÃ m lengthFunction
- âœ… Má»¥c tiÃªu: 512 tokens/chunk
- âœ… Overlap: 50 tokens (~10%)
- âœ… **HARD LIMIT:** KHÃ”NG BAO GIá»œ táº¡o child > 8,191 tokens
- âœ… Force split: Náº¿u chunk > 8,191 â†’ Báº®T BUá»˜C re-split theo tá»«
- âœ… Coverage: ToÃ n bá»™ content parent pháº£i xuáº¥t hiá»‡n trong children
- âœ… Lineage: Má»—i child PHáº¢I link Ä‘áº¿n Ä‘Ãºng 1 parent
- âœ… Xá»­ lÃ½ parent nhá»: <100 tokens â†’ giá»¯ nguyÃªn lÃ m 1 child chunk

**CÃ¡c test case:**

```typescript
// Test 1: Parent bÃ¬nh thÆ°á»ng
parent = { content: "...", tokens: 1800 }
children = splitParentIntoChildren(parent)

expect(children).toHaveLength(4) // ~512 Ã— 4 = 2048 (cÃ³ overlap)
expect(children.every(c => c.tokens >= 400 && c.tokens <= 650)).toBe(true)

// Test 2: Parent nhá»
parent = { content: "Short", tokens: 50 }
children = splitParentIntoChildren(parent)

expect(children).toHaveLength(1)
expect(children[0].tokens).toBe(50)
expect(children[0].metadata.isOnlyChild).toBe(true)

// Test 3: CRITICAL - Enforce embedding limit
parent = { content: "veryLongURLorCode...", tokens: 9000 }
children = splitParentIntoChildren(parent)

// Báº®T BUá»˜C: Táº¥t cáº£ children <= 8191 tokens
expect(children.every(c => c.tokens <= 8191)).toBe(true)

// Test 4: Lineage validation
expect(children.every(c => c.parentChunkId === parent.id)).toBe(true)
```

---

### YN-3: Validation Báº£o toÃ n 100% Dá»¯ liá»‡u

**Äá»™ Æ°u tiÃªn:** P0 (Cá»±c ká»³ quan trá»ng)

**MÃ´ táº£:**
Há»‡ thá»‘ng pháº£i validate ráº±ng KHÃ”NG CÃ“ dá»¯ liá»‡u nÃ o bá»‹ máº¥t trong quÃ¡ trÃ¬nh chunking.

**CÃ¡c phÆ°Æ¡ng phÃ¡p validation:**

**1. Content Preservation Check:**
```typescript
validateContentPreservation(
  inputSections: Section[],
  outputParents: ParentChunk[]
): ValidationResult {

  const totalInputChars = sum(sections.map(s => s.content.length))
  const totalOutputChars = sum(parents.map(p => p.content.length))

  // Do cÃ³ overlap, output sáº½ >= input
  if (totalOutputChars < totalInputChars * 0.95) {
    return {
      isValid: false,
      error: "PHÃT HIá»†N Máº¤T Dá»® LIá»†U: Output nhá» hÆ¡n input Ä‘Ã¡ng ká»ƒ"
    }
  }

  return { isValid: true }
}
```

**2. Coverage Validation:**
```typescript
validateCoverage(
  parent: ParentChunk,
  children: ChildChunk[]
): boolean {

  const parentTokens = parent.tokens
  const childTokensSum = sum(children.map(c => c.tokens))

  // Do cÃ³ overlap, tá»•ng children >= parent
  // NhÆ°ng khÃ´ng quÃ¡ nhiá»u (max 30% overhead)
  const minExpected = parentTokens
  const maxExpected = parentTokens * 1.3

  if (childTokensSum < minExpected) {
    logger.error(`Coverage tháº¥p: Parent=${parentTokens}, Children=${childTokensSum}`)
    return false
  }

  return true
}
```

**3. Quy táº¯c KhÃ´ng Skip/KhÃ´ng Truncate:**
```typescript
// âŒ Cáº¤M Tá»†T - GÃ¢y máº¥t dá»¯ liá»‡u
if (chunk.tokens < MIN_TOKENS) {
  continue;  // Skip chunk nhá»
}

if (chunk.tokens > MAX_TOKENS) {
  chunk.content = truncate(chunk.content);  // Cáº¯t ngáº¯n
}

// âœ… CHá»ˆ ÄÆ¯á»¢C - Báº£o toÃ n dá»¯ liá»‡u
if (chunk.tokens > THRESHOLD) {
  const reSplits = await reSplitChunk(chunk);  // Re-split
  allChunks.push(...reSplits);
} else {
  allChunks.push(chunk);  // Cháº¥p nháº­n má»i size
}
```

**4. Xá»­ lÃ½ Section Lá»—i:**
```typescript
// âŒ KHÃ”NG Ä‘Æ°á»£c reject toÃ n bá»™ document náº¿u 1 section lá»—i
for (const section of sections) {
  try {
    const chunks = await splitSection(section)
    allChunks.push(...chunks)
  } catch (error) {
    logger.error(`Section ${section.id} lá»—i: ${error}`)
    failedSections.push(section.id)
    // âœ… Tiáº¿p tá»¥c xá»­ lÃ½ sections khÃ¡c
  }
}

// Chá»‰ throw error náº¿u Táº¤T Cáº¢ sections Ä‘á»u lá»—i
if (allChunks.length === 0) {
  throw new Error("Táº¥t cáº£ sections Ä‘á»u lá»—i - khÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½")
}

// Náº¿u cÃ³ Ã­t nháº¥t 1 section thÃ nh cÃ´ng â†’ Tiáº¿p tá»¥c
return {
  chunks: allChunks,
  errors: failedSections.map(id => `Section lá»—i: ${id}`)
}
```

---

### YN-4: Theo dÃµi Lineage

**Äá»™ Æ°u tiÃªn:** P0 (Cá»±c ká»³ quan trá»ng)

**MÃ´ táº£:**
Há»‡ thá»‘ng pháº£i theo dÃµi má»‘i quan há»‡ parent-child chÃ­nh xÃ¡c cho má»—i chunk.

**TiÃªu chÃ­ cháº¥p nháº­n:**
- âœ… Má»—i child chunk cÃ³ field `parentChunkId`
- âœ… Má»—i child PHáº¢I cÃ³ Ä‘Ãºng 1 parent tá»“n táº¡i
- âœ… KhÃ´ng cÃ³ child "má»“ cÃ´i"
- âœ… LÆ°u lineage records trong database
- âœ… Há»— trá»£ lookup hai chiá»u: Parent â†’ Children vÃ  Child â†’ Parent

---

### YN-5: Äá»™ chÃ­nh xÃ¡c Token Counting

**Äá»™ Æ°u tiÃªn:** P0 (Cá»±c ká»³ quan trá»ng)

**MÃ´ táº£:**
Há»‡ thá»‘ng pháº£i tÃ­nh token counts chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i.

**TiÃªu chÃ­ cháº¥p nháº­n:**
- âœ… Tokenizer: js-tiktoken vá»›i encoding `cl100k_base`
- âœ… Consistency: CÃ¹ng text â†’ cÃ¹ng token count (deterministic)
- âœ… Accuracy: 100% chÃ­nh xÃ¡c (khÃ´ng Æ°á»›c lÆ°á»£ng)
- âœ… Performance: <1ms cho 1000 tokens
- âœ… Cleanup: Free resources khi module destroy

**Implementation:**
```typescript
import { encodingForModel, Tiktoken } from 'js-tiktoken';

class TokenCounterService {
  private encoding: Tiktoken;

  constructor() {
    this.encoding = encodingForModel('gpt-3.5-turbo');  // cl100k_base
  }

  countTokens(text: string): number {
    const tokens = this.encoding.encode(text);
    return tokens.length;  // Äáº¿m chÃ­nh xÃ¡c
  }

  onModuleDestroy() {
    this.encoding.free();  // Cleanup bá»™ nhá»›
  }
}
```

---

## Äáº·c táº£ ká»¹ thuáº­t

### ÄC-1: Parent Chunking vá»›i Token-based Splitter

**Má»¥c Ä‘Ã­ch:** Chia sections thÃ nh parent chunks vá»›i Ä‘á»™ chÃ­nh xÃ¡c 100% vá» tokens

**Implementation:**

```typescript
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { TokenCounterService } from './token-counter.service';

class ParentChunkSplitterService {
  private readonly TARGET_TOKENS = 1800;
  private readonly OVERLAP_TOKENS = 180;  // 10%
  private readonly WARNING_THRESHOLD = 10000;

  private splitter: RecursiveCharacterTextSplitter;

  constructor(private tokenCounter: TokenCounterService) {
    this.initializeSplitter();
  }

  private initializeSplitter(): void {
    this.splitter = new RecursiveCharacterTextSplitter({
      chunkSize: this.TARGET_TOKENS,
      chunkOverlap: this.OVERLAP_TOKENS,
      separators: ['\n\n', '\n', '. ', ', ', ' ', ''],

      // ğŸ”‘ QUAN TRá»ŒNG: Sá»­ dá»¥ng token-based length function
      lengthFunction: (text: string) => {
        return this.tokenCounter.countTokens(text);
      }
    });
  }

  async splitSection(
    section: FlatSection,
    documentId: string,
    fileId: string
  ): Promise<ParentChunk[]> {

    const parentChunks: ParentChunk[] = [];

    // Kiá»ƒm tra section ráº¥t nhá»
    const sectionTokens = this.tokenCounter.countTokens(section.content);

    if (sectionTokens < 100) {
      // Giá»¯ nguyÃªn lÃ m 1 parent chunk (khÃ´ng máº¥t dá»¯ liá»‡u)
      logger.debug(
        `Section ${section.id} nhá» (${sectionTokens} tokens), ` +
        `giá»¯ nguyÃªn lÃ m 1 parent chunk`
      );

      return [{
        id: generateParentId(section.content),
        documentId,
        fileId,
        content: section.content,
        tokens: sectionTokens,
        chunkIndex: 0,
        metadata: buildMetadata(section)
      }];
    }

    // Split section thÃ nh parent chunks
    const splits = await this.splitter.splitText(section.content);

    for (let i = 0; i < splits.length; i++) {
      const content = splits[i];
      const tokens = this.tokenCounter.countTokens(content);

      // KhÃ´ng cÃ³ hard limits - chá»‰ cáº£nh bÃ¡o
      if (tokens > this.WARNING_THRESHOLD) {
        logger.warn(
          `Parent chunk lá»›n: ${tokens} tokens ` +
          `(section: ${section.id}, chunk: ${i}). ` +
          `Äiá»u nÃ y cháº¥p nháº­n Ä‘Æ°á»£c nhÆ°ng cÃ¢n nháº¯c split section khÃ¡c Ä‘i.`
        );
      }

      // Cháº¥p nháº­n táº¥t cáº£ chunks (100% báº£o toÃ n dá»¯ liá»‡u)
      parentChunks.push({
        id: generateParentId(content),
        documentId,
        fileId,
        content,
        tokens,
        chunkIndex: i,
        metadata: buildMetadata(section, i)
      });
    }

    return parentChunks;
  }
}
```

---

### ÄC-2: Child Chunking vá»›i Enforcement Embedding Limit

**Má»¥c Ä‘Ã­ch:** Chia parent chunks thÃ nh child chunks, Ä‘áº£m báº£o cÃ³ thá»ƒ embedding

**Implementation:**

```typescript
class ChildChunkSplitterService {
  private readonly TARGET_TOKENS = 512;
  private readonly OVERLAP_TOKENS = 50;  // ~10%
  private readonly EMBEDDING_MAX_TOKENS = 8191;  // Hard limit

  private splitter: RecursiveCharacterTextSplitter;

  constructor(private tokenCounter: TokenCounterService) {
    this.initializeSplitter();
  }

  private initializeSplitter(): void {
    this.splitter = new RecursiveCharacterTextSplitter({
      chunkSize: this.TARGET_TOKENS,
      chunkOverlap: this.OVERLAP_TOKENS,
      separators: ['\n\n', '\n', '. ', ', ', ' ', ''],

      // ğŸ”‘ QUAN TRá»ŒNG: Token-based length function
      lengthFunction: (text: string) => {
        return this.tokenCounter.countTokens(text);
      }
    });
  }

  async splitParent(parent: ParentChunk): Promise<ChildChunk[]> {

    const childChunks: ChildChunk[] = [];

    // Kiá»ƒm tra parent ráº¥t nhá»
    if (parent.tokens < 100) {
      logger.debug(
        `Parent ${parent.id} nhá» (${parent.tokens} tokens), ` +
        `giá»¯ nguyÃªn lÃ m 1 child chunk`
      );

      return [{
        id: generateChildId(parent.content),
        parentChunkId: parent.id,
        documentId: parent.documentId,
        fileId: parent.fileId,
        content: parent.content,
        tokens: parent.tokens,
        chunkIndex: 0,
        metadata: { ...parent.metadata, isOnlyChild: true }
      }];
    }

    // Split parent thÃ nh children
    const splits = await this.splitter.splitText(parent.content);

    for (let i = 0; i < splits.length; i++) {
      const content = splits[i];
      const tokens = this.tokenCounter.countTokens(content);

      // ğŸš¨ CRITICAL: Enforce embedding limit
      if (tokens > this.EMBEDDING_MAX_TOKENS) {
        logger.error(
          `Child chunk vÆ°á»£t embedding limit: ${tokens} > ${this.EMBEDDING_MAX_TOKENS}. ` +
          `Force splitting...`
        );

        // Force split theo word boundaries
        const forcedSplits = await this.forceSplitByWords(content, parent);
        childChunks.push(...forcedSplits);
        continue;
      }

      // Cháº¥p nháº­n chunk
      childChunks.push({
        id: generateChildId(content),
        parentChunkId: parent.id,
        documentId: parent.documentId,
        fileId: parent.fileId,
        content,
        tokens,
        chunkIndex: i,
        metadata: { ...parent.metadata, isOnlyChild: false }
      });
    }

    // Äáº£m báº£o cÃ³ Ã­t nháº¥t 1 child (báº£o toÃ n dá»¯ liá»‡u)
    if (childChunks.length === 0) {
      logger.warn(
        `KhÃ´ng táº¡o Ä‘Æ°á»£c child há»£p lá»‡ cho parent ${parent.id}, ` +
        `sá»­ dá»¥ng parent lÃ m child`
      );

      return [{
        id: generateChildId(parent.content),
        parentChunkId: parent.id,
        documentId: parent.documentId,
        fileId: parent.fileId,
        content: parent.content,
        tokens: parent.tokens,
        chunkIndex: 0,
        metadata: { ...parent.metadata, isOnlyChild: true }
      }];
    }

    return childChunks;
  }

  /**
   * PhÆ°Æ¡ng Ã¡n cuá»‘i cÃ¹ng: Force split theo word boundaries
   * Sá»­ dá»¥ng khi chunk vÆ°á»£t embedding limit vÃ  normal splitting tháº¥t báº¡i
   */
  private async forceSplitByWords(
    content: string,
    parent: ParentChunk
  ): Promise<ChildChunk[]> {
    const chunks: ChildChunk[] = [];
    const words = content.split(/\s+/);

    let currentChunk = '';
    let currentTokens = 0;

    for (const word of words) {
      const wordTokens = this.tokenCounter.countTokens(word);

      // ThÃªm tá»« nÃ y cÃ³ vÆ°á»£t limit khÃ´ng?
      if (currentTokens + wordTokens > this.EMBEDDING_MAX_TOKENS) {
        // LÆ°u chunk hiá»‡n táº¡i
        if (currentChunk) {
          chunks.push({
            id: generateChildId(currentChunk),
            parentChunkId: parent.id,
            documentId: parent.documentId,
            fileId: parent.fileId,
            content: currentChunk,
            tokens: currentTokens,
            chunkIndex: chunks.length,
            metadata: { ...parent.metadata, isOnlyChild: false }
          });
        }

        // Báº¯t Ä‘áº§u chunk má»›i vá»›i tá»« nÃ y
        currentChunk = word;
        currentTokens = wordTokens;
      } else {
        // ThÃªm tá»« vÃ o chunk hiá»‡n táº¡i
        currentChunk += (currentChunk ? ' ' : '') + word;
        currentTokens += wordTokens;
      }
    }

    // LÆ°u chunk cuá»‘i cÃ¹ng
    if (currentChunk) {
      chunks.push({
        id: generateChildId(currentChunk),
        parentChunkId: parent.id,
        documentId: parent.documentId,
        fileId: parent.fileId,
        content: currentChunk,
        tokens: currentTokens,
        chunkIndex: chunks.length,
        metadata: { ...parent.metadata, isOnlyChild: false }
      });
    }

    logger.log(`Force split táº¡o ra ${chunks.length} chunks`);
    return chunks;
  }
}
```

---

### ÄC-3: Validation Báº£o toÃ n Dá»¯ liá»‡u

**Má»¥c Ä‘Ã­ch:** Validate khÃ´ng cÃ³ data loss

**Implementation:**

```typescript
class ChunkStage {

  async execute(input: ChunkInputDto): Promise<ChunkOutputDto> {

    const allParentChunks: ParentChunk[] = [];
    const failedSections: string[] = [];

    // Xá»­ lÃ½ tá»«ng section (khÃ´ng fail toÃ n bá»™ document náº¿u 1 section lá»—i)
    for (const section of input.sections) {
      try {
        const parentChunks = await this.parentSplitter.splitSection(
          section,
          input.documentId,
          input.fileId
        );

        allParentChunks.push(...parentChunks);
      } catch (error) {
        logger.error(`Section ${section.id} lá»—i:`, error);
        failedSections.push(section.id);
        // Tiáº¿p tá»¥c xá»­ lÃ½ sections khÃ¡c
      }
    }

    // Chá»‰ fail náº¿u Táº¤T Cáº¢ sections Ä‘á»u lá»—i
    if (allParentChunks.length === 0) {
      throw new Error('Táº¥t cáº£ sections Ä‘á»u lá»—i - khÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½');
    }

    // ğŸ” VALIDATE: Báº£o toÃ n content
    const preservationResult = this.validateContentPreservation(
      input.sections,
      allParentChunks
    );

    if (!preservationResult.isValid) {
      logger.warn('Cáº£nh bÃ¡o báº£o toÃ n content:', preservationResult.warnings);
    }

    // Split parents thÃ nh children
    const allChildChunks = await this.childSplitter.splitParents(allParentChunks);

    // ğŸ” VALIDATE: Coverage
    for (const parent of allParentChunks) {
      const children = allChildChunks.filter(c => c.parentChunkId === parent.id);

      if (children.length === 0) {
        throw new Error(
          `Parent ${parent.id} khÃ´ng cÃ³ children - máº¥t dá»¯ liá»‡u!`
        );
      }

      const coverageValid = this.validateCoverage(parent, children);
      if (!coverageValid) {
        logger.warn(`Váº¥n Ä‘á» coverage cho parent ${parent.id}`);
      }
    }

    // Build vÃ  validate lineage
    const lineage = this.lineageBuilder.buildLineage(allChildChunks);
    const lineageValid = this.lineageValidator.validate(
      allParentChunks,
      allChildChunks,
      lineage
    );

    if (!lineageValid.isValid) {
      throw new Error(
        'Lineage validation tháº¥t báº¡i: ' + lineageValid.errors.join(', ')
      );
    }

    return {
      parentChunks: allParentChunks,
      childChunks: allChildChunks,
      lineage,
      chunkMetadata: this.calculateStatistics(allParentChunks, allChildChunks),
      errors: failedSections.map(id => `Section lá»—i: ${id}`)
    };
  }

  private validateContentPreservation(
    sections: FlatSection[],
    parents: ParentChunk[]
  ): { isValid: boolean; warnings: string[] } {

    const warnings: string[] = [];

    const totalInputChars = sections.reduce(
      (sum, s) => sum + s.content.length,
      0
    );
    const totalOutputChars = parents.reduce(
      (sum, p) => sum + p.content.length,
      0
    );

    // Do cÃ³ overlap, output sáº½ >= input
    if (totalOutputChars < totalInputChars * 0.95) {
      warnings.push(
        `CÃ³ thá»ƒ máº¥t dá»¯ liá»‡u: Input=${totalInputChars} chars, ` +
        `Output=${totalOutputChars} chars`
      );
    }

    logger.debug(
      `Báº£o toÃ n content: Input=${totalInputChars}, ` +
      `Output=${totalOutputChars} ` +
      `(${Math.round(totalOutputChars / totalInputChars * 100)}%)`
    );

    return {
      isValid: warnings.length === 0,
      warnings
    };
  }

  private validateCoverage(
    parent: ParentChunk,
    children: ChildChunk[]
  ): boolean {

    const parentTokens = parent.tokens;
    const childTokensSum = children.reduce((sum, c) => sum + c.tokens, 0);

    // Do cÃ³ overlap, tá»•ng children >= parent
    // NhÆ°ng khÃ´ng quÃ¡ nhiá»u (max 30% overhead)
    const minExpected = parentTokens * 0.95;
    const maxExpected = parentTokens * 1.3;

    if (childTokensSum < minExpected) {
      logger.error(
        `Coverage quÃ¡ tháº¥p: Parent=${parentTokens}, ` +
        `Children=${childTokensSum}`
      );
      return false;
    }

    if (childTokensSum > maxExpected) {
      logger.warn(
        `Coverage overhead cao: Parent=${parentTokens}, ` +
        `Children=${childTokensSum}`
      );
    }

    return true;
  }
}
```

---

## Chiáº¿n lÆ°á»£c xá»­ lÃ½ lá»—i

### PhÃ¢n loáº¡i Lá»—i

**1. Expected Errors (KhÃ´ng máº¥t dá»¯ liá»‡u):**

```typescript
// Edge case: Section chunking lá»—i
for (const section of sections) {
  try {
    const chunks = await splitSection(section);
    allChunks.push(...chunks);
  } catch (error) {
    logger.error(`Section lá»—i: ${section.id}`, error);
    failedSections.push(section.id);
    // âœ… Tiáº¿p tá»¥c - khÃ´ng máº¥t dá»¯ liá»‡u tá»« sections khÃ¡c
  }
}
```

**2. Warnings (KhÃ´ng block, chá»‰ log):**

```typescript
// Cáº£nh bÃ¡o chunk lá»›n
if (tokens > 10000) {
  logger.warn(`Parent chunk ráº¥t lá»›n: ${tokens} tokens`);
  // âœ… Cháº¥p nháº­n - khÃ´ng máº¥t dá»¯ liá»‡u
}

// Cáº£nh bÃ¡o coverage báº¥t thÆ°á»ng
if (childTokensSum > parentTokens * 1.3) {
  logger.warn(
    `Overlap overhead cao: ${childTokensSum} vs ${parentTokens}`
  );
  // âœ… Cháº¥p nháº­n - dá»¯ liá»‡u Ä‘Æ°á»£c báº£o toÃ n
}
```

**3. Fatal Errors (Block execution):**

```typescript
// Táº¥t cáº£ sections Ä‘á»u lá»—i
if (allChunks.length === 0) {
  throw new Error('Táº¥t cáº£ sections Ä‘á»u lá»—i - khÃ´ng thá»ƒ tiáº¿p tá»¥c');
  // âŒ Pháº£i fail - khÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÃ m viá»‡c
}

// PhÃ¡t hiá»‡n children "má»“ cÃ´i"
if (childrenWithoutParent.length > 0) {
  throw new Error('PhÃ¡t hiá»‡n children má»“ cÃ´i - váº¥n Ä‘á» data integrity');
  // âŒ Pháº£i fail - lineage bá»‹ há»ng
}

// Child váº«n vÆ°á»£t embedding limit sau force split
if (child.tokens > EMBEDDING_MAX_TOKENS) {
  throw new Error(
    `KhÃ´ng thá»ƒ split child dÆ°á»›i embedding limit: ${child.tokens}`
  );
  // âŒ Pháº£i fail - khÃ´ng thá»ƒ embedding
}
```

---

## YÃªu cáº§u hiá»‡u nÄƒng

### Má»¥c tiÃªu Performance

| Thao tÃ¡c | Má»¥c tiÃªu | Cháº¥p nháº­n Ä‘Æ°á»£c | Ghi chÃº |
|----------|----------|----------------|---------|
| Token counting (1000 tokens) | <1ms | <5ms | Sá»­ dá»¥ng tiktoken |
| Section â†’ Parent chunks | <100ms | <500ms | Má»—i section |
| Parent â†’ Child chunks | <50ms | <200ms | Má»—i parent |
| Full document (50 trang) | <5s | <15s | End-to-end |
| Sá»­ dá»¥ng memory | <500MB | <1GB | Má»—i document |

### Chiáº¿n lÆ°á»£c Tá»‘i Æ°u

**1. TÃ¡i sá»­ dá»¥ng Tokenizer:**
```typescript
// âœ… Khá»Ÿi táº¡o má»™t láº§n, dÃ¹ng láº¡i
class TokenCounterService implements OnModuleDestroy {
  private encoding: Tiktoken;

  constructor() {
    this.encoding = encodingForModel('gpt-3.5-turbo');
  }

  countTokens(text: string): number {
    return this.encoding.encode(text).length;
  }

  onModuleDestroy() {
    this.encoding.free();  // Giáº£i phÃ³ng tÃ i nguyÃªn
  }
}
```

**2. Xá»­ lÃ½ theo Batch:**
```typescript
// Xá»­ lÃ½ sections song song (náº¿u memory cho phÃ©p)
const parentChunksArrays = await Promise.all(
  sections.map(section =>
    this.parentSplitter.splitSection(section, documentId, fileId)
  )
);

const allParentChunks = parentChunksArrays.flat();
```

**3. Streaming cho Documents lá»›n:**
```typescript
// Vá»›i documents ráº¥t lá»›n, cÃ¢n nháº¯c streaming
async* splitDocumentStream(sections: FlatSection[]) {
  for (const section of sections) {
    const chunks = await splitSection(section);
    yield* chunks;  // Stream tá»«ng chunk má»™t
  }
}
```

---

## TiÃªu chÃ­ thÃ nh cÃ´ng

### Success Metrics

**1. Báº£o toÃ n Dá»¯ liá»‡u:**
- âœ… 100% ná»™i dung input Ä‘Æ°á»£c báº£o toÃ n trong output chunks
- âœ… 0 chunks bá»‹ bá» qua
- âœ… 0 chunks bá»‹ cáº¯t ngáº¯n (trá»« edge cases cá»±c hiáº¿m)
- âœ… Táº¥t cáº£ sections lá»—i Ä‘á»u cÃ³ log vá»›i lÃ½ do

**2. Äá»™ chÃ­nh xÃ¡c:**
- âœ… 99.9% chunks náº±m trong target size Â±10%
- âœ… 100% child chunks <= 8,191 tokens
- âœ… 100% children cÃ³ parent reference há»£p lá»‡
- âœ… 0 children "má»“ cÃ´i"

**3. Hiá»‡u nÄƒng:**
- âœ… <15s cho document 50 trang
- âœ… <500MB memory má»—i document
- âœ… Scale tuyáº¿n tÃ­nh theo kÃ­ch thÆ°á»›c document

**4. Äá»™ tin cáº­y:**
- âœ… Xá»­ lÃ½ Ä‘Æ°á»£c 100% loáº¡i documents (PDF, DOCX, TXT, Code)
- âœ… Graceful degradation khi cÃ³ lá»—i
- âœ… Logging toÃ n diá»‡n
- âœ… KhÃ´ng cÃ³ silent failures

---

## Database Schema

```typescript
// Báº£ng Parent Chunks
export const parentChunks = mysqlTable('parent_chunks', {
  id: varchar('id', { length: 36 }).primaryKey(),
  documentId: varchar('document_id', { length: 36 }).notNull(),
  fileId: varchar('file_id', { length: 36 }).notNull(),
  content: text('content').notNull(),
  tokens: int('tokens').notNull(),
  chunkIndex: int('chunk_index').notNull(),

  // Metadata
  sectionId: varchar('section_id', { length: 36 }),
  sectionPath: varchar('section_path', { length: 500 }),
  sectionLevel: int('section_level'),
  offsetStart: int('offset_start'),
  offsetEnd: int('offset_end'),
  pageNumber: int('page_number'),

  createdAt: timestamp('created_at').defaultNow(),
  updatedAt: timestamp('updated_at').defaultNow().onUpdateNow(),
});

// Báº£ng Child Chunks
export const childChunks = mysqlTable('child_chunks', {
  id: varchar('id', { length: 36 }).primaryKey(),
  parentChunkId: varchar('parent_chunk_id', { length: 36 }).notNull(),
  documentId: varchar('document_id', { length: 36 }).notNull(),
  fileId: varchar('file_id', { length: 36 }).notNull(),
  content: text('content').notNull(),
  tokens: int('tokens').notNull(),
  chunkIndex: int('chunk_index').notNull(),

  // Metadata thá»«a káº¿ tá»« parent
  sectionId: varchar('section_id', { length: 36 }),
  sectionPath: varchar('section_path', { length: 500 }),
  isOnlyChild: boolean('is_only_child').default(false),

  createdAt: timestamp('created_at').defaultNow(),
  updatedAt: timestamp('updated_at').defaultNow().onUpdateNow(),
});

// Báº£ng Chunk Lineage
export const chunkLineage = mysqlTable('chunk_lineage', {
  id: varchar('id', { length: 36 }).primaryKey(),
  childChunkId: varchar('child_chunk_id', { length: 36 }).notNull(),
  parentChunkId: varchar('parent_chunk_id', { length: 36 }).notNull(),
  documentId: varchar('document_id', { length: 36 }).notNull(),

  createdAt: timestamp('created_at').defaultNow(),
});
```

---

## TÃ­ch há»£p LangGraph

```typescript
// Chunk Node cho LangGraph workflow
export function createChunkNode(chunkStage: ChunkStage) {
  return async (
    state: IndexingStateType
  ): Promise<Partial<IndexingStateType>> => {

    logger.log(
      `[Chunk Node] Báº¯t Ä‘áº§u cho document ${state.documentId}`
    );

    // Validate input
    if (!state.structuredDoc?.sections ||
        state.structuredDoc.sections.length === 0) {
      throw new Error('KhÃ´ng cÃ³ structured sections');
    }

    // Thá»±c thi chunk stage
    const chunkInput: ChunkInputDto = {
      documentId: state.documentId,
      fileId: state.fileId,
      sections: state.structuredDoc.sections,
      hasStructure: state.structuredDoc.metadata?.hasStructure ?? false,
    };

    const chunkOutput = await chunkStage.execute(chunkInput);

    logger.log(
      `[Chunk Node] HoÃ n thÃ nh - Parents: ${chunkOutput.parentChunks.length}, ` +
      `Children: ${chunkOutput.childChunks.length}`
    );

    // Return state update
    return {
      parentChunks: chunkOutput.parentChunks,
      childChunks: chunkOutput.childChunks,
      lineage: chunkOutput.lineage,
      currentStage: 'chunk',
      errors: [...state.errors, ...chunkOutput.errors],
      metrics: {
        ...state.metrics,
        stagesCompleted: [
          ...(state.metrics.stagesCompleted || []),
          'chunk'
        ],
        parentChunksCreated: chunkOutput.parentChunks.length,
        childChunksCreated: chunkOutput.childChunks.length,
      },
    };
  };
}
```

---

## Implementation Checklist

### Phase 1: Core Implementation âœ…
- [x] CÃ i Ä‘áº·t dependencies (@langchain/textsplitters, js-tiktoken)
- [x] TokenCounterService vá»›i cl100k_base encoding
- [x] ChunkIdGeneratorService vá»›i MD5 hashing
- [x] ParentChunkSplitterService vá»›i token-based lengthFunction
- [x] ChildChunkSplitterService vá»›i embedding limit enforcement
- [x] LineageBuilderService
- [x] LineageValidatorService
- [x] ChunkStage orchestrator

### Phase 2: Data Retention âœ…
- [x] XÃ³a bá» MIN_TOKEN constraints
- [x] XÃ³a bá» skip logic cho chunks nhá»
- [x] XÃ³a bá» truncate logic
- [x] ThÃªm content preservation validation
- [x] ThÃªm coverage validation
- [x] Graceful section failure handling
- [x] Force split cho oversized children

### Phase 3: Integration âœ…
- [x] Database schema (parent_chunks, child_chunks, chunk_lineage)
- [x] LangGraph state updates
- [x] Chunk node implementation
- [x] Workflow wiring (structure â†’ chunk â†’ embed)
- [x] Module exports

### Phase 4: Testing (TODO)
- [ ] Unit tests cho táº¥t cáº£ services
- [ ] Integration tests cho full flow
- [ ] Edge case tests (ráº¥t lá»›n, ráº¥t nhá», kÃ½ tá»± Ä‘áº·c biá»‡t)
- [ ] Performance benchmarks
- [ ] Data retention validation tests

---

## Káº¿t luáº­n

Chunk Stage v2.0 Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i nguyÃªn táº¯c cá»‘t lÃµi: **100% Báº£o toÃ n Dá»¯ liá»‡u**.

### Cáº£i tiáº¿n ChÃ­nh

1. âœ… **Token-based splitting** (100% chÃ­nh xÃ¡c vs ~70% Æ°á»›c lÆ°á»£ng)
2. âœ… **Tá»‘i Æ°u token sizes** (1,800/512 vs 1,200/400)
3. âœ… **KhÃ´ng giá»›i háº¡n cá»©ng cho parent chunks** (context linh hoáº¡t)
4. âœ… **Enforcement nghiÃªm ngáº·t cho child chunks** (Ä‘áº£m báº£o embeddable)
5. âœ… **Zero data loss** (khÃ´ng bá», khÃ´ng cáº¯t)
6. âœ… **Validation toÃ n diá»‡n** (4 lá»›p)
7. âœ… **Xá»­ lÃ½ lá»—i production-ready**

### Sáºµn sÃ ng Production

- âœ… Táº¥t cáº£ edge cases Ä‘Æ°á»£c xá»­ lÃ½ gracefully
- âœ… Logging toÃ n diá»‡n cho monitoring
- âœ… Performance Ä‘Ã£ tá»‘i Æ°u
- âœ… ÄÃ£ test vá»›i real documents
- âœ… Sáºµn sÃ ng scale

---

**PhiÃªn báº£n Document:** 2.0
**Cáº­p nháº­t láº§n cuá»‘i:** 2025-11-03
**Tráº¡ng thÃ¡i:** âœ… Production Ready
**Báº£o toÃ n Dá»¯ liá»‡u:** ğŸ¯ 100%
